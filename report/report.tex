\documentclass[a4paper, 11pt, twocolumn]{IEEEtran}

\usepackage{graphicx}
\usepackage[scale=.8]{geometry}
\usepackage{microtype}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{lipsum} 


\title{Some title for a multimodal chess interface}
\author{Giuseppina Iannotti, and Davide Marincione}
\date{11th of June, 2024}

\begin{document}
    \maketitle

    \section{Introduction}
    In this report, we present the design, implementation, and testing, of a multimodal interface for playing chess. The interface allows the user to play with either mouse movements, hand gestures or voice commands. The user can switch between the different modalities at any time during the game. It is implemented in \texttt{python} and uses the \texttt{pygame} library for the audio-visual inteface, \texttt{opencv} and \texttt{mediapipe} for hand gesture recognition, and the \texttt{dragonfly} library for voice commands. The interface has been tested with a group of 12 users, and the results show that the multimodal interface is more engaging and fun to use than a traditional mouse-based interface.

    \section{Coding, Graphics \& Audio}
    \paragraph*{Libraries} A chess program is quite complex to make, and since writing one would've been a project in itself, we have decided to use the \texttt{python-chess} library. This choice enabled us to abstract away the complexity of the game, and to focus on making the interface and the interaction. \texttt{python-chess} is used all over the project's code, to make checks and queries to display the correct information, but also to interact with the chess engine and to make moves.\\For the graphics, audio and event handling, we used the \texttt{pygame} library, which is a set of \texttt{python} modules designed for writing small video games. It is simple yet very powerful, as it allows to draw shapes, images and text on the screen, to play sounds, and to handle user input. Admittedly, it can be quite slow for medium to large-scale projects, as its main drawback is the graphics rendering, which is done via old-fashioned blitting. But, for a project such as ours, it was more than enough.

    \paragraph*{Programming paradigm} For all its usefullness, \texttt{pygame} only provides basic functionalities and none of the data structures and systems used in writing videogames. Things that, to a certain extent, we needed for our project. So we decided to go for an OOP approach, defining ever more refined objects, building on top of more abstract ones. This tactic proved to be extremely successful, as in the later parts of the project there were a couple of instances in which we needed a new class, and we were able to define it without much hassle.
    \begin{figure*}
        \centering
        \includegraphics[width=.8\textwidth]{images/uml.pdf}
        \caption{UML description of the basic structure of the program.}
        \label{fig:uml}
    \end{figure*}
    \paragraph*{Basic structure} Inspired by many game engines (Unity, Godot, etc.), our main class is the \texttt{Object} class, which is the base class for most of the elements in our program \ref{fig:uml}. It is defined to be as generic as possible: an object in 2d space which, being in a parent-child hierarchy, can have its absolute position changed by the parent, while maintaining its relative position to it. Internally it has a class-wide \texttt{OBJECT\_COUNTER} member which is used to give each \texttt{Object} its \texttt{id}, used to recognize it among the other objects.
    \paragraph*{Rendering} We then have the \texttt{Renderable} class, which inherits from \texttt{Object}. And, with it, a \texttt{Renderer} class, which is a singleton that keeps track of all the \texttt{Renderable} objects and renders them. As such, the \texttt{Renderable} class has a \texttt{draw()} method, a \texttt{set\_visible()} method and an \texttt{order} attribute, which is used to determine the order in which the objects are rendered. The \texttt{Renderer} class has a \texttt{step()} method, which is called every frame, and it renders all the \texttt{Renderable} objects in the correct order.
    \paragraph*{Clickables} The \texttt{Clickable} class inherits from \texttt{Renderable} and is used to define objects that can be clicked (in our simple system, we don't have invisible but clickable objects). Like with \texttt{Renderer} and \texttt{Renderable}, we have a \texttt{Clicker} singleton, that keeps track of all the \texttt{Clickable}s. It too has a method called every frame, named \texttt{highlight()}, which determines which \texttt{Clickable} would be clicked if a "click" event was called. When that happens, its \texttt{execute\_click()} method is called, which runs the \texttt{click()} and \texttt{declick()}\footnote{The \texttt{declick()} method is run when the left mouse button is released, instead of pressed.} methods that each \texttt{Clickable} must have. This way, we can define different behaviours for different objects.\\Other than that, the \texttt{Clickable} class has a \texttt{enable\_highlight()} method and \texttt{disable\_highlight()} method, which are used to enable and disable the highlight state of the object, respectively. Furthermore, it has a \texttt{hold\_draw()} method, used by the \texttt{Cursor} class to draw the object when it is being held (if it can be held).
    \begin{figure}
        \centering
        \includegraphics[width=.45\textwidth]{images/cursor_comparison.png}
        \caption{The cursor, with different backgrounds.}
        \label{fig:cursor}
    \end{figure}
    \paragraph*{Cursor} The \texttt{Cursor} class is a singleton that inherits from \texttt{Renderable}. As the name suggests it is used to draw the cursor on the screen, and to keep track of the object that is being held. As described in the next sections it can be driven either by mouse or by hand gestures, and it is referenced by the \texttt{Clicker} as it handles which object is being held (as only \texttt{Clickable}s are holdable). We decided to make the \texttt{Cursor} a \texttt{Renderable} such as to make it different from the os mouse cursor; we felt that it would be better to have a custom cursor such as to give a distinct visual feedback, especially when the cursor is driven by hand gestures. With this class we have made our first interface choice, as the cursor is designed to be a fairly big cross symbol, which distorts the colors it is drawn upon; making it always visible, no matter the background \ref{fig:cursor}. Initially we had thought of using
    \begin{equation}
        c^* = 255 - c,
    \end{equation}
    but we found this to be insufficient, as one can expect most colors to sit in the middle of the color spectrum, thus getting a situation in which the cursor is invisible on most colors, as
    \begin{equation}
        255 - 127 = 128.
    \end{equation}
    To avoid this, we resorted to
    \begin{equation}
        c^* = (c + 128)\ \mathrm{mod}\ 256,
    \end{equation}
    which assures us to always get a color that is distant enough from the original.
    \paragraph*{Text} As we sometimes needed to display text on the screen, we have defined the \texttt{FloatingText} class, which inherits from \texttt{Renderable}. It exploits \texttt{pygame}'s \texttt{Font} class, which is used to render text on the screen, and thanks to that it can dynamically change the displayed text and the color (as the \texttt{Font} class enables that).
    \paragraph*{Board} The \texttt{Board} class is a singleton that inherits from \texttt{Renderable}. Internally, it has a \texttt{board} attribute, which contains a \texttt{chess.Board} object, used to make moves and to check the state of the game. Furthermore, it has a reference to 64 \texttt{GUISquare} objects, which inherit from \texttt{Clickable} and handle the drawing of pieces on the screen, and their interaction with the user. To build a satisfying interface, \texttt{Board} has a number of features:
    \begin{itemize}
        \item It enables a red square under the king when it is in check.
        \item It marks a square with green when it gets selected (as a move can be done by doing two clicks, the first one to select the piece, the second one to select the destination), and highlights in the same way all of the possible moves.
        \item It plays a sound when a move is done, and a different one when a king gets in check.
    \end{itemize}
    \begin{figure}
        \centering
        \includegraphics[width=.45\textwidth]{images/promotion_bubble.png}
        \caption{How the promotion bubble looks like.}
        \label{fig:promotion}
    \end{figure}
    \paragraph*{Promotion} The \texttt{Board} recognizes when a pawn reaches the last opposite row, when this happens, before executing the move by calling its internal \texttt{chess.Board} object, it makes a popup element appear, the \texttt{PromotionBubble} \ref{fig:promotion}. This is a \texttt{Renderable} which contains four \texttt{PromotionButton}s, which inherit from \texttt{Clickable}. Each of these buttons represent a piece to which the pawn can be promoted to. When a button is clicked, the \texttt{PromotionBubble} calls the \texttt{Board} to finalize the move and to promote the pawn.
    \paragraph*{Game loop, events and AI} The main loop of the program can be found in \texttt{chess\_main.py}. After the initialization of all the objects and systems, it enters a loop which can be roughly described as follows:
    \begin{enumerate}
        \item Update the cursor position with the latest mouse or hand position, and run \texttt{clicker.highlight(cursor\_pos)}. If a hand gesture is detected, push a mouse event.
        \item Resolve events, such as mouse clicks, key presses (quit game, takebacks), and moves done (for the AI).
        \item Resolve voice commands.
        \item Run \texttt{renderer.step()}.
        \item Run metrics recorder.
    \end{enumerate}
    During the game, the player competes against Stockfish 16 (which, in our tests, we limited); we easily were able to do this thanks to the \texttt{python-chess} library, which has an built-in interface to communicate with a multitude of chess engines. The AI is run during event handling, as our \texttt{Board} emits a custom \texttt{TURN\_DONE} event which we use to discern when the AI has to make a move.
    \paragraph*{Audio} In the game, we only have three different sounds: two of which we already mentioned, played by the \texttt{Board}, and the last one played in response to malformed voice commands. The sound played when a move is done is a simple "wood-on-wood" sound, which reminisces the sound of a piece being moved on a wooden chessboard. The other two sounds, being alarm/attention sounds, are intentionally synthetic beeps, to make them stand out.
    \paragraph*{Game recording} At the end of development, we wanted to collect quantitative data about the tests. To do this, we implemented a recording system that saves multiple metrics about the runtime; such as the amount of moves made during a modality, the distance traveled with the hand and the number of utterances by the user. In the Experiments section of this report, we'll talk more about this.
    \section{Hand tracking and gestures}
    \paragraph*{Libraries} The hand tracking and gesture recognition system is enabled by the \texttt{opencv} and \texttt{mediapipe} libraries. The first is used to capture the video stream of the camera, and the latter is used to detect and track the hand. The rest of the computation (mapping the hand position to the cursor position, detecting gestures, etc.) is done by our code.
    \paragraph*{Hand detection loop} We design a singleton \texttt{HandDetector} class, which runs its own loop on a separate thread with respect to the main system. This is done to avoid blocking the main loop, and to give as smooth an experience as possible. The loop can be described as follows:
    \begin{enumerate}
        \item Capture the video stream.
        \item If the video stream has stopped, stop the loop.
        \item Flip the video stream horizontally (if needed).
        \item If an async call to the \texttt{mediapipe} library is not running, execute one with the current frame.
    \end{enumerate}
    The async call is done to avoid blocking the loop as the \texttt{mediapipe} library can be quite slow, and the check for no other async calls is done to not accumulate multiple async calls (which, on a slow laptop, can cause the system to crash). Once the async call gets executed, \texttt{mediapipe} runs a custom callback which we design to processes the resulting hand landmarks into the \texttt{Hand} class, which we then store in the \texttt{HandDetector}.
    \paragraph*{Normalizing hands} The landmarks $\mathbf{L}\in\mathrm{R}^3$ that \texttt{mediapipe} produces live in an approximated screen space. This is fairly useful when mapping the hand to the screen space $\mathrm{R}^2$, as we can just disregard the third dimension. But, when one wants to recognize hand gestures while being invariant to different people's hands, it must be modified. Therefore, before storing them, we normalize the landmarks such as to minimize the differences between different hands, their position and orientation.\\
    First and foremost, we extract the palm center $p$ by calculating a weighted average (handmade by us) over the base of the fingers and the wrist
    \begin{equation}
        p = \frac{\mathbf{L}_1}{2} + \frac{\mathbf{L}_6 + \mathbf{L}_{10} + \mathbf{L}_{14} + \mathbf{L}_{18}}{8}.
    \end{equation}
    We extract the palm width $w$, as it is invariant to different hand positions,
    \begin{equation}
        w = ||\mathbf{L}_6 - \mathbf{L}_{18}||_2,
    \end{equation}
    and we get the relative landmarks $\overline{\mathbf{L}}$ by normalizing the landmarks with respect to the palm center and the palm width,
    \begin{equation}
        \overline{\mathbf{L}} = \frac{\mathbf{L} - p}{w}.
    \end{equation}
    \begin{figure}
        \centering
        \includegraphics[width=.33\textwidth]{images/norm_hand.png}
        \caption{Right hand in $\mathbf{L}^\star$ doing a grab gesture.}
        \label{fig:norm_hand}
    \end{figure}
    We could've stopped here, as $\overline{\mathbf{L}}$ is enough to recognize the gestures that we designed for the project; but at the time we weren't sure if we would've needed a more expressive representation for more complex gestures. Because of this, we decided to apply a change of basis to $\overline{\mathbf{L}}$ such as to get $\mathbf{L}^\star$, where the internal palm normal is the $z$ axis, the palm width is the $x$ axis (with the positive direction being that of the pinky, regardless of the handedness), and the $y$ axis as their cross product (which, when calculated, should be the direction of the fingers). To do this, we first get the palm normal
    \begin{equation}
        \mathbf{n}_{\mathrm{palm}} = (1 - \mathbf{2}_{\textrm{left}})\frac{(\mathbf{L}_6 - \mathbf{L}_1)\times(\mathbf{L}_{18} - \mathbf{L}_1)}{||(\mathbf{L}_6 - \mathbf{L}_1)\times(\mathbf{L}_{18} - \mathbf{L}_1)||_2},
    \end{equation}
    then we get the pinky normal,
    \begin{equation}
        \mathbf{n}_{\mathrm{pinky}} = (1 - \mathbf{2}_{\textrm{left}})\frac{\mathbf{n}_{\mathrm{palm}}\times(\mathbf{L}_{10} - \mathbf{L}_1)}{||\mathbf{n}_{\mathrm{palm}}\times(\mathbf{L}_{10} - \mathbf{L}_1)||_2}.
    \end{equation}
    and the fingers' normal,
    \begin{equation}
        \mathbf{n}_{\mathrm{fingers}} = (1 - \mathbf{2}_{\textrm{left}})\frac{\mathbf{n}_{\mathrm{pinky}}\times \mathbf{n}_{\mathrm{palm}}}{||\mathbf{n}_{\mathrm{pinky}}\times \mathbf{n}_{\mathrm{palm}}||_2}.
    \end{equation}
    This produces a basis $\mathbf{B} = \left[\mathbf{n}_{\mathrm{pinky}}, \mathbf{n}_{\mathrm{fingers}}, \mathbf{n}_{\mathrm{palm}}\right]$ that is invariant to the hand's orientation, and that gives explicit meaning to the axes. Finally, we produce $\mathbf{L}^\star$,
    \begin{equation}
        \mathbf{L}^\star = \overline{\mathbf{L}}\mathbf{B}^T.
    \end{equation}
    \paragraph*{Gesture recognition} At the outset, we wanted to design two different gestures:
    \begin{enumerate}
        \item A grabbing motion, which would let the user drag-n-drop the pieces. Composed when the thumb and the index are close enough to each other.
        \item A tapping motion, which would let the user click the squares. Composed by a forward flick of the wrist.
    \end{enumerate}
    As we were developing the system, we found that the tapping motion was too hard to recognize reliably (as it doesn't require a change in the landmarks' relative positions). At the same time, we found that the grabbing motion we were developing was actually an already established gesture (used in devices like the Apple Vision Pro) used both for single-click gestures and for drag-n-drop ones. Because of this, we decided to stick with this single gesture, and to use it in a manner akin to pressing the left mouse button.\\
    To recognize the gesture, we design
    \begin{algorithmic}[1]
        \State \textbf{input} \texttt{prev\_click}
        \State $m \gets ||\mathbf{L}^\star_{\mathrm{thumb}} - \mathbf{L}^\star_{\mathrm{index}}||_2$
        \State $d \gets \frac{\mathbf{L}^\star_{\mathrm{thumb}}\cdot\mathbf{L}^\star_{\mathrm{index}}}{||\mathbf{L}^\star_{\mathrm{thumb}}||_2 ||\mathbf{L}^\star_{\mathrm{index}}||_2}$
        \If{\texttt{prev\_click}}
            \State \textbf{return} $m < \alpha_\gamma \land d > \beta_\gamma$
        \Else
            \State \textbf{return} $m < \alpha \land d > \beta$
        \EndIf
    \end{algorithmic}
    where $\alpha$ and $\beta$ are the normal thresholds, and $\alpha_\gamma$ and $\beta_\gamma$ are the thresholds for the hysteresis effect, when the user is already doing a grabbing motion (to avoid dropping the piece when the user is moving it).
    \paragraph*{Hand-cursor mapping} As described in the coding section, at each main loop we retrieve the latest hand-cursor mapping, and, if its timestamp is later than the latest mouse timestamp, we use that. We run the hand-cursor mapping during this main loop, using the latest data given by the hand tracking async code.
    While being fairly precise, \texttt{mediapipe}'s hand tracking system is not perfect: many frames can pass between one detection and the other, tracking is noisy and, finally, the system can lose track of the hand. Therefore we had to build a robust mechanism that could handle these issues.\\
    First and foremost, we use the palm $p$ as the point to map the cursor to, and we rescale and clamp its position to a $[m, M]$ space, such that we can make it easier for the user to reach the edges of the window.
    We do this by
    \begin{equation}
        r = \mathrm{clip}_{[m,M]}(\frac{p - m}{M - m}).
    \end{equation}
    Secondly, we use both the latest position $r_t$ and the previous one $r_{t-1}$ (if these are present, that is). Furthermore, we keep a cursor position $c$ internal to \texttt{HandDetector}, which is independent of the cursor position in the main loop. Then we define an algorithm that updates $c$ and returns it, complete with whether a grab or release event happened (whether the user has started or stopped grabbing).\\
    The algorithm is robust in the sense that it tries to avoid experience-ruining events, such as:
    \begin{enumerate}
        \item The cursor jumping around because of noisy tracking, $c$ moves at a constant rate and can either be updated via bilinear or linear interpolation, based on whether $r_{t-1}$ is present, such as to make its movement smooth (at the cost of being a bit less responsive).
        \item The cursor moving even when the hand is still. This can either happen because humans have a natural tremor in their hands, which can be picked up by the hand tracking system, or because the tracking is noisy. To avoid this, we only update the cursor if the hand has moved a certain distance.
        \item A grabbed piece being dropped for whatever reason, while the user is still keeping a grab gesture. This is avoided by keeping a list of the most recent detections and checking if even one of them is a grab gesture.
    \end{enumerate}
    With all of this combined we achieved a system that, in spite of all the noise and issues, provides a smooth experience. We describe the tests and the results in the Experiments section.
    \section{Voice commands}
    To enhance user interaction with the application, our project aims to integrate voice commands. Playing chess using verbal instructions can provide a more engaging experience and offer advantages over traditional input methods. In fact, It can be particularly beneficial for users with accessibility needs or those in hands-free environments.
    \paragraph*{Dragonfly} To implement voice commands, we use the \emph{dragonfly} library, a Python package that facilitates the creation of voice command scripts. Dragonfly allows for the intuitive definition of complex command grammars and simplifies the processing of recognition results by treating speech commands and grammar objects as first-class Python objects. Additionally, It provides a unified front-end interface that seamlessly integrates with Kaldi as a back-end speech recognition engine.
    \begin{table*}[h]
        \centering
        \caption{Rules and Specifications}
        \begin{tabularx}{\textwidth}{|l | X|}
            \hline
            \textbf{Rule Name} & \textbf{Specification} \\
            \hline
            Move Rule & \texttt{"move ( [ <src\_piece>] | [ <src\_piece> [<prep> <src\_square>] to <tgt\_square>] | [ [<prep>] <src\_square> to <tgt\_square>]) [and promote to <prm\_piece>]"} \\
            \hline
            Capture Rule & \texttt{"capture (<tgt\_piece> [<prep> <tgt\_square>] | <tgt\_square>) [with (<src\_piece> [<prep> <src\_square>] | <src\_square>)] [and promote to <prm\_piece>]"}\\
            \hline
            Promote Rule & \texttt{"promote [(<src\_piece> | <src\_square>)] to <prm\_piece>"} \\
            \hline 
            Castle Rule & \texttt{"(castle <special\_direction> | <special\_direction> castle)"} \\
            \hline 
            Piece Rule & \texttt{"<src\_piece> ( [<prep>] <tgt\_square> |in <src\_square> <verb> [<prep>] ( <tgt\_square> | <tgt\_piece> [in <tgt\_square>] ) | <verb> [<prep> <src\_square>]( [<prep>] <tgt\_square> | <tgt\_piece>  [in <tgt\_square>])) [and promote to <prm\_piece>]"}\\
            \hline
            Square Rule & \texttt{"<src\_square> <verb> ( [<prep>] <tgt\_square> | <tgt\_piece> [<prep> <tgt\_square>])"}\\
            \hline
        \end{tabularx}
        \label{tab:rules}
    \end{table*}
    \paragraph*{Rules} Vocal instructions are defined using a set of rules, which specify the structure of the commands and associate them with specific actions within the game. Each rule is instantiated as a \emph{CompoundRule} class, which facilitates the straightforward creation of rules based on a single compound specification. This rule class has the following parameters to customize its behavior:
    \begin{itemize}
        \item \emph{spec} : Compound specification for the rules root element 
        \item \emph{extras} : Extras elements referenced from the compound spec. It includes choices for prepositions, pieces, and squares.
    \end{itemize}
    \paragraph*{Grammar} Rules are combined into a Grammar object, which is subsequently loaded into the Dragonfly engine. The grammar object is tasked with processing voice input and comparing it to the established rules. Upon identifying a match, the associated action is executed, such as moving a chess piece or promoting a pawn. Specifically, our Chess Grammar comprises the following set of rules, detailed in \ref{tab:rules} : 
    \begin{itemize}
        \item \texttt{Move Rule}: Defines a structured syntax pattern for recognizing actions initiated with the verb \emph{"Move"}. Examples of commands recognized by this rule are: "Move E7 to E8 and promote to Queen", "Move Queen on H8 to H7" 
        \item \texttt{Capture Rule}: Establishes a syntax pattern for recognizing and processing actions starting with the verb \emph{"Capture"}. This pattern allows for the identification of target and source pieces and squares, along with any promotion involved. Examples of actions recognized by this rule are: "Capture Bishop on E4 with Queen", "Capture Pawn with E6" 
        \item \texttt{Promotion Rule}: Specifies the syntax for recognizing and processing pawn promotion actions, beginning with the verb \emph{"Promote"}. It identifies the source piece or square and specifies the promotion piece. Examples of instructions recognized by this rule are: "Promote Pawn to Queen", "Promote to Queen"
        \item \texttt{Castle Rule}: Defines the syntax for recognizing and processing castling commands, focusing on identifying the direction of the castle (kingside or queenside). Examples of commands recognized by this rule are : "Kingside Castle", "Castle Queenside"
        \item \texttt{Piece Rule}: Establishes a structured syntax pattern for interpreting commands starting with a piece name, handling various actions involving a specified piece. Examples of actions recognized by this rule are: "Pawn capture Knight and promote to Queen", "Queen capture Bishop"
        \item \texttt{Square Rule}: Specifies a pattern for recognizing commands that involve specifying source square and, subsequently, a target square or a target piece on the chessboard. This rule is designed to handle actions that involve moving or capturing a piece from a specific square to another square or piece. Example of this commands recognized by this rule are: "A2 move to A3", "E6 capture Bishop on E7"
    \end{itemize} 
    Each rule contains a method for processing recognition, tasked with managing the identified voice commands. Upon recognition, it extracts the pertinent elements from the outcome and assembles a Command object embodying the chess move. This Command object, encapsulating details such as the action verb, source and target pieces, and their corresponding chessboard squares, serves as a container for the information conveyed by the voice commands. Subsequently, this command is transmitted to the Speech Manager for execution.
    \paragraph*{Speech Manager} It is responsible for interpreting vocal commands and translating them into actionable moves within the chess application. It acts as a bridge between the speech recognition system and the chess engine, ensuring seamless interaction between the user's spoken instructions and the game state. Upon receiving vocal commands from the speech recognition system, it analyzes each command to extract relevant information such as the verb (e.g., move, capture), source and target pieces, and destination squares. The function validates the extracted commands against the legal moves available on the chessboard. It filters the available moves based on the provided details and determines the most appropriate move based on the context. Moreover, It handles special cases such as castling and pawn promotion separately to ensure accurate interpretation and execution of these complex maneuvers. To prevent outdated or delayed commands from being executed, the function incorporates a timeout mechanism to discard commands that exceed a specified time threshold. As for the \texttt{HandDetector} class, the \texttt{Speech Manager} runs on a separate thread with respect to the main system. This is done to avoid blocking the main loop, and ensures efficient processing of vocal commands within the application.
    \section{Experiments and Results} As previously mentioned, at the end of each game, we aim to collect both qualitative and quantitative data related to it. 
    \paragraph*{Recording System} For this purpose, we implemented a recording system that captures various metrics about the runtime. The collected data, detailed in the JSON file, offers a comprehensive view of the chess game, capturing player moves and AI action. This systematic approach to data collection not only aids in understanding user behavior and preferences but also provides valuable insights into the accuracy and effectiveness of the proposed gameplay modalities.The JSON file is organized into two primary sections, namely the \emph{Player Actions} and the \emph{AI moves}. \\ Each player action object contains:
    \begin{itemize}
        \item \texttt{Action Start} : Timestamp marking the beginning of the action
        \item \texttt{Action Type} : Type of action (e.g., "speech", "hand", "mouse")
        \item \texttt{Utterances} (Optional) : Number of speech utterances if the action type is "speech"
        \item \texttt{Hand Distance} (Optional) : Distance traveled by the hand if the action type is "hand" 
        \item \texttt{Mouse Distance} (Optional) : Distance traveled by the mouse if the action type is "mouse"
        \item \texttt{Down and Up Botton} (Optional) : Button press counts if the action type is "hand" or "mouse"
        \item \texttt{Moves} : Array of moves made during the action. Each move is detailed by piece type, starting position, ending position, and any promotion piece, if present 
        \item \texttt{Action End} : Timestamp marking the end of the action    
    \end{itemize}
    Instead, the AI's moves are stored in a list, with each move represented as for Player's Moves.
    \paragraph*{Metrics} Recording files, containing player's and AI's moves, were processed to extract relevant metrics. We conduct descriptive statistical analyses on our dataset to facilitate more informed interpretations. This involve generating a series of graphs aimed at providing quantitative and qualitative descriptions of specific metrics. \\ \textbf{Note} : Periods of time during which individuals used to speak without executing any commands were recorded based on the last modality used. Given that the Gesture Modality was the most utilized, the data and distributions reported are influenced accordingly. \\\texttt{Time Spent Using Each Modality} : 
    \begin{figure}
        \centering
        \includegraphics[width=.5\textwidth]{images/total_time.png}
        \caption{Time spent on using each modality}
        \label{fig:total_time}
    \end{figure}
    In Fig. \ref{fig:total_time}, we illustrate the distribution of time spent by users on each modality. It is calculated by summing the duration of all actions performed using that modality. The plot clearly demonstrates that users predominantly allocate more time to Gesture actions compared to Voice interactions. Concerning the former, no user has reported fatigue using the handtracking mechanism, keeping in mind the tests were usually less than 10 minutes long. On the other side, altough using voice input has garnered particular interest too, its use was limited. This was due both to the greater familiarity with playing by hand and to linguistic difficulties encountered by some users. Instead, only a few users have opted to utilize the mouse as an input modality. Its limited usage may be attributed to users' curiosity and inclination to explore the functioning of alternative modalities. However, in the few cases in which the users chose to use the mouse, they were able to pick it up instantly and without any issues. This reassures us of the solid baseline implemented.\\\texttt{Actions Per Minute} :
    \begin{figure}
        \centering
        \includegraphics[width=.5\textwidth]{images/actions_per_minutes.png}
        \caption{Number of Actions per Minute performed by users across different modalities}
        \label{fig:actions_per_minutes}
    \end{figure}
    Fig. \ref{fig:actions_per_minutes} illustrates the frequency of actions performed by users across the different modalities during chess gameplay. It is a measurement of how efficient and fast users perform commands using the different modalities. We can see that the highest bars are those representing speech interactions. However, this distribution is skewed due to the aformentioned \textbf{Note}. \\\texttt{Total Actions} : 
    \begin{figure}
        \centering
        \includegraphics[width=.5\textwidth]{images/total_actions.png}
        \caption{Number of total actions performed by users across different modalities}
        \label{fig:total_actions}
    \end{figure}
    Fig. \ref{fig:total_actions} illustrates the total number of actions performed by users across different recordings, categorized by their type of modality. The plot indicates that the number of actions performed by users is relatively high, including both legal and illegal commands. As shown, the majority of users opted for gesture-based interactions, being consistent with Fig. \ref{fig:total_time}, in which we see that users have spent more time using the gesture modality. \\\texttt{Legal Actions} : 
    \begin{figure}
        \centering
        \includegraphics[width=.5\textwidth]{images/legal_actions.png}
        \caption{Number of legal actions performed by users across different modalities}
        \label{fig:legal_actions}
    \end{figure}
    Fig. \ref{fig:legal_actions} shows the distribution of legal actions performed by users across different recordings, categorized by their type of modality. \\
    From the previous plots, we can understand the number of legal actions is relatively small compared to the total number of actions performed. For this purpose, we are particularly interested in analyzing the error rate of the introduced modalities, computed as: 
    \begin{equation}
        1 - \frac{\mathrm{total\_legal\_actions}}{\mathrm{total\_actions}}
    \end{equation}
    \texttt{Gesture Error Rate} : 
    \begin{figure}
        \centering
        \includegraphics[width=.5\textwidth]{images/hand_error_rate.png}
        \caption{Hand Error Rate }
        \label{fig:hand_error_rate}
    \end{figure}
    At the beginning of our experiments, there were issues related to the quality of the hand input, see Fig. \ref{fig:hand_error_rate}. Specifically, the piece grabbing was too sticky. This problem was later resolved by removing the hysteresis effect. Additionally, we observed that no user complained about the visibility of the cursor, which we consider a positive indication of its design. As anticipated, it was unnecessary to match hand and cursor in a 1:1 mapping, as users focused more on the cursorâ€™s position rather than their hand's position.
    \texttt{Speech Error Rate} : 
    \begin{figure}
        \centering
        \includegraphics[width=.5\textwidth]{images/speech_error_rate.png}
        \caption{Speech Error Rate }
        \label{fig:speech_error_rate}
    \end{figure}
    Several factors influences the speech error rate, shown at \ref{fig:speech_error_rate}, including users' accents and pronunciation. It is important to note that the implemented grammars cover only a subset of all possible phrases that can be formulated in a chess game. However, this limitation did not pose a significant problem, as the phrases spoken by the users were within the scope of the implemented rules. Additionally, during the experiments, it was observed that the speech recognizer often struggled to distinguish between 'f' and 'e' and 'a'. To avoid comprehension errors, we implemented also the NATO phonetic alphabet, a system that assigns standardized code words to each letter of the alphabet, like 'Alpha' for 'A', 'Bravo' for 'B' and so on. Despite few players used it, it is highly recommended. Moreover, as previously mentioned, many users used to talk while switching between modalities. Their conversations about matters unrelated to the application were often recognized as commands, which, being illegal, triggered a warning sound. This aspect was less appreciated by our users. However, it is assumed that during a chess game, players generally do not engage in unrelated conversations.
    \paragraph*{Game Over} During the experiments, the need to introduce a "takeback" feature was highlighted. This feature allows users to revert to the previous action, proving especially useful when the gesture or voice recognizer inadvertently processes an unintended action due to a potential misinterpretation. Users report also that the end of the game status was not clear enough, as they were not paying attention to the initial "Press R to restart" message. This led to the implementation of a more visible game over message at the end of the game directly on the board. 
    \begin{figure}
        \centering
        \includegraphics[width=.45\textwidth]{images/interface.png}
        \caption{Final Image of the Interface at the end of the development}
        \label{fig:interface}
    \end{figure} 
    The interface at the end of the development is shown at Fig. \ref{fig:interface}
    \section{Conclusions} The application provides a novel and interactive way to play chess, offering users the flexibility to choose their preferred input modality. Overall, the application garnered significant interest from the users, providing them with an enjoyable experience. However, despite being the least utilized input method, the mouse remains the preferred mode due to its convenience and suitability for various situations. The collected results indicate potential, but it is important to note that the calculated metrics are biased by several factors, including the limited amount of data collected, lengthy conversations unrelated to chess during the game, and the users' accents and pronunciation. Looking ahead, future work could involve further refining the hand tracking and gesture recognition system, as well as expanding the range of voice commands supported by the application and using a more robust speech recognizer to improve accuracy and reliability. Additionally, one future project might involve implementing chess gameplay through eye-gaze tracking, trying to add a novel dimension to digital chess gameplay. 
\end{document}
